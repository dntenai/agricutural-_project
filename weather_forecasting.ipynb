{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eZ5cvwA3yDM"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set working directory\n",
        "working_directory = \"C:/Users/ADMIN/Documents/Agriculture Project\"\n",
        "os.chdir = working_directory"
      ],
      "metadata": {
        "id": "6cHZhRvP32ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "df = pd.read_csv(\"C:/Users/ADMIN/Documents/Agriculture Project/city_temperature.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Fr6zwkOO32r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if all the cities has the data for a full range\n",
        "df['City'].value_counts()"
      ],
      "metadata": {
        "id": "0L7xrlYK33Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time series for a random city\n",
        "chennai = df[df[\"City\"] == \"Chennai (Madras)\"]\n",
        "chennai.head()"
      ],
      "metadata": {
        "id": "g4qmbmBt33vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#do any missing values for the years\n",
        "chennai[\"Year\"].value_counts()"
      ],
      "metadata": {
        "id": "3gbEevyG34e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fill\n",
        "chennai[\"AvgTemperature\"] = np.where(chennai[\"AvgTemperature\"] == -99, np.nan, chennai[\"AvgTemperature\"])\n",
        "chennai.isnull().sum()"
      ],
      "metadata": {
        "id": "dWn4AXXM3428"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for null after filling\n",
        "chennai[\"AvgTemperature\"] = chennai[\"AvgTemperature\"].ffill()\n",
        "chennai.isnull().sum()"
      ],
      "metadata": {
        "id": "9DLx9eSG35Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create date column\n",
        "chennai.dtypes\n",
        "chennai[\"Time_steps\"] = pd.to_datetime((chennai.Year*10000 + chennai.Month*100 + chennai.Day).apply(str),format='%Y%m%d')\n",
        "chennai.head()"
      ],
      "metadata": {
        "id": "0Dx9sL8Z36Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
        "    \"\"\"to plot the series\"\"\"\n",
        "    plt.plot(time[start:end], series[start:end], format)\n",
        "    plt.xlabel(\"Year\")\n",
        "    plt.ylabel(\"Temprature\")\n",
        "    plt.grid(True)"
      ],
      "metadata": {
        "id": "L3DVJbVc36fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time series for the entire time\n",
        "time_step = chennai[\"Time_steps\"].tolist()\n",
        "temprature = chennai[\"AvgTemperature\"].tolist()\n",
        "\n",
        "series = np.array(temprature)\n",
        "time = np.array(time_step)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time, series)"
      ],
      "metadata": {
        "id": "7vqeWER64iJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the recent year\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time[-365:], series[-365:])"
      ],
      "metadata": {
        "id": "w-bKgEUK4iiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split data\n",
        "split_time = 8000\n",
        "time_train = time[:split_time]\n",
        "x_train = series[:split_time]\n",
        "time_valid = time[split_time:]\n",
        "x_valid = series[split_time:]"
      ],
      "metadata": {
        "id": "lUHB0veG4tr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert numpy array to tensor\n",
        "series1 = tf.expand_dims(series, axis=-1)\n",
        "ds = tf.data.Dataset.from_tensor_slices(series1[:20])\n",
        "for val in ds:\n",
        "    print(val.numpy())"
      ],
      "metadata": {
        "id": "-ZpNkOf44uDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#group 5 observation into 1\n",
        "dataset = ds.window(5, shift=1)\n",
        "for window_dataset in dataset:\n",
        "    for val in window_dataset:\n",
        "        print(val.numpy(), end=\" \")\n",
        "    print()"
      ],
      "metadata": {
        "id": "AJ3rtLpP43dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop variables with no grouping dataset = ds.window(5, shift=1, drop_remainder=True)\n",
        "for window_dataset in dataset:\n",
        "    for val in window_dataset:\n",
        "        print(val.numpy(), end=\" \")\n",
        "    print()"
      ],
      "metadata": {
        "id": "5Ftohu9O434a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#group 5 observation into 1 tensor varaible\n",
        "dataset = ds.window(5, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
        "for window in dataset:\n",
        "    print(window.numpy())"
      ],
      "metadata": {
        "id": "ViCxacLp5BWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split variables into X an y\n",
        "dataset = ds.window(5, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
        "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
        "for x,y in dataset:\n",
        "    print(x.numpy(), y.numpy())"
      ],
      "metadata": {
        "id": "scBYoeP15ByR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffle\n",
        "dataset = ds.window(5, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
        "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
        "dataset = dataset.shuffle(buffer_size=10)\n",
        "for x,y in dataset:\n",
        "    print(x.numpy(), y.numpy())"
      ],
      "metadata": {
        "id": "lAwVeZjY5TqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#group into mini batches\n",
        "dataset = ds.window(5, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
        "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
        "dataset = dataset.shuffle(buffer_size=10)\n",
        "dataset = dataset.batch(2).prefetch(1)\n",
        "for x,y in dataset:\n",
        "    print(\"x = \", x.numpy())\n",
        "    print(\"y = \", y.numpy())\n",
        "    print(\"*\"*25)"
      ],
      "metadata": {
        "id": "nP695VR35h51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 60\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000"
      ],
      "metadata": {
        "id": "Jd4e3aCg5iUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    \"\"\"\n",
        "    To create a window dataset given a numpy as input\n",
        "\n",
        "    Returns: A prefetched tensorflow dataset\n",
        "    \"\"\"\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "A1zM-oO_5iqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #build the model\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(51)\n",
        "np.random.seed(51)\n",
        "train_set = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=shuffle_buffer_size)\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n",
        "                      strides=1, padding=\"causal\",\n",
        "                      activation=\"relu\",\n",
        "                      input_shape=[None, 1]),\n",
        "  tf.keras.layers.LSTM(60, return_sequences=True),\n",
        "  tf.keras.layers.LSTM(60, return_sequences=True),\n",
        "  tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 400)\n",
        "])\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-7, momentum=0.9)\n",
        "model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "history = model.fit(train_set,epochs=500)"
      ],
      "metadata": {
        "id": "p3vo-AGe5ouR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogx(history.history[\"learning_rate\"], history.history[\"loss\"])\n",
        "plt.axis([1e-8, 1e-4, 0, 60])"
      ],
      "metadata": {
        "id": "ex4B7S6j5pO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_forecast(model, series, window_size):\n",
        "    \"\"\"\n",
        "    Given a model object and a series for it to predict, this function will return the prediction\n",
        "    \"\"\"\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "    ds = ds.batch(32).prefetch(1)\n",
        "    forecast = model.predict(ds)\n",
        "    return forecast"
      ],
      "metadata": {
        "id": "UPzKSH8-5u7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\n",
        "rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]"
      ],
      "metadata": {
        "id": "qwVVXu9s5vXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time_valid, x_valid)\n",
        "plot_series(time_valid, rnn_forecast)"
      ],
      "metadata": {
        "id": "0CI9InjA55K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot series\n",
        "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
        "    plt.plot(time[start:end], series[start:end], format)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "\n",
        "\n",
        "# Forecasting\n",
        "rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\n",
        "rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time_valid, x_valid)\n",
        "plot_series(time_valid[window_size:], rnn_forecast)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "evWwLCv-6G58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mae\n",
        "tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()"
      ],
      "metadata": {
        "id": "ynmpFBgc6HR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss\n",
        "loss=history.history['loss']\n",
        "\n",
        "epochs=range(len(loss)) # Get number of epochs\n",
        "\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r')\n",
        "plt.title('Training loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Loss\"])\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "zoomed_loss = loss[200:]\n",
        "zoomed_epochs = range(200,500)\n",
        "\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(zoomed_epochs, zoomed_loss, 'r')\n",
        "plt.title('Training loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Loss\"])\n",
        "\n",
        "plt.figure()"
      ],
      "metadata": {
        "id": "D9GS_Yk66ZXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r_EfGYfO6Z0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7libkAol6hZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U9JdFrKu6hxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yg7dgMMA6iS5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}